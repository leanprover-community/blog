---
author: 'R√©my Degenne'
category: 'Tutorial'
date: 2024-10-02 16:41:48 UTC+02:00
description: ''
has_math: true
link: ''
slug: basic-probability-in-lean
tags: ''
title: Basic probability in Lean
type: text
---

How do I define a probability space and two independent random variables in Lean? Should I use `IsProbabilityMeasure` or `ProbabilityMeasure`?
How do I condition on an event?

This post answers these and other simple questions about how to express probability concepts using Mathlib.
We will mostly not discuss theorems, but focus on definitions. The goal is to have enough knowledge about the definitions in Mathlib related to probability to state probability theory lemmas.

<!-- TEASER_END -->

The code examples will not mention imports: use `import Mathlib` in a project that depends on Mathlib (and then prune the imports with `#min_imports` if you like).
Many probability related notations are defined in the file Probability/Notation.
Including the following two lines at the beginning of a file after the imports is generally a good idea to work with probability:
```lean
open MeasureTheory ProbabilityTheory
open scoped ENNReal
```
The first line opens namespaces, which means that we will be able to omit any `MeasureTheory.` prefix from lemma names.
The second line makes some notations available. We'll talk about that further down.



# Probability spaces and probability measures

First, in order to work on probability we need a measurable space.
We can define a probability measure on such a space as follows.
```lean
variable {Œ© : Type*} [MeasurableSpace Œ©] {P : Measure Œ©} [IsProbabilityMeasure P]
```
The class `MeasurableSpace Œ©` defines a sigma-algebra on `Œ©`. We then introduced a measure `P` and specified that it should be a probability measure.
If we want to work on `‚Ñù` or another well known type the typeclass inference system will find `[MeasurableSpace ‚Ñù]` on its own. We can write simply
```lean
variable {P : Measure ‚Ñù} [IsProbabilityMeasure P]
```

With the code above, we can introduce several probability measures on the same space. When using lemmas and definitions about those measures, we will need to specify which measure we are talking about.
For example, the variance of a random variable `X` with respect to that measure `P` will be `variance X P`.

But perhaps we just want a space with a canonical probability measure, which would ideally be the one used without us having to tell Lean explicitly.
That can be done with the `MeasureSpace` class. A `MeasureSpace` is a `MeasurableSpace` with a canonical measure, called `volume`.
The probability library of Mathlib defines a notation `‚Ñô` for that measure. We still need to tell that we want it to be a probability measure though.
```lean
variable {Œ© : Type*} [MeasureSpace Œ©] [IsProbabilityMeasure (‚Ñô : Measure Œ©)]
```
Remark 1: in the code above we can't write only `[IsProbabilityMeasure ‚Ñô]` because Lean would then not know to which space the default measure `‚Ñô` refers to.
That will not be necessary when we use `‚Ñô` in proofs because the context will be enough to infer `Œ©`.

Remark 2: a lemma written for `P : Measure Œ©` in a `MeasurableSpace Œ©` will apply for the special measure `‚Ñô` in a `MeasureSpace Œ©`, but the converse is not true.
Mathlib focuses on generality, hence uses the `MeasurableSpace` spelling for its lemmas. In another context, the convenience of `MeasureSpace` may be preferable.


## `IsProbabilityMeasure` vs `ProbabilityMeasure`

The examples above used `{P : Measure Œ©} [IsProbabilityMeasure P]` to define a probability measure. That's the standard way to do it.
Mathlib also contains a type `ProbabilityMeasure Œ©`. The goal of that type is to work on the set of probability measures on `Œ©`.
In particular, that type comes with a topology, the topology of convergence in distribution (weak convergence of measures).
If we don't need to work with that topology, `{P : Measure Œ©} [IsProbabilityMeasure P]` should be preferred.


## Probability of events

A `Measure` can be applied to a set like a function, and returns a value in `ENNReal` (denoted by `‚Ñù‚â•0‚àû`, available after `open scoped ENNReal`).
```lean
example (P : Measure ‚Ñù) (s : Set ‚Ñù) : ‚Ñù‚â•0‚àû := P s
```
The type `‚Ñù‚â•0‚àû` represents the nonnegative reals and infinity: the measure of a set is a nonnegative real number which in general may be infinite.
Measures can in general take infinite values, but since our `‚Ñô` is a probabilty measure, it actually takes only values up to 1.
`simp` knows that a probability measure is finite and will use the lemmas `measure_ne_top` or `measure_lt_top` to prove that `‚Ñô s ‚â† ‚àû` or `‚Ñô s < ‚àû`.
The operations on `‚Ñù‚â•0‚àû` are not as nicely behaved as on `‚Ñù`: `‚Ñù‚â•0‚àû` is not a ring. For example, subtraction truncates to zero.
If one finds that lemma `lemma_name` used to transform an equation does not apply to `‚Ñù‚â•0‚àû`, a good thing to try is to find a lemma named like `ENNReal.lemma_name_of_something` and use that instead (it will typically require that one variable is not infinite).

For many lemmas to apply, the set `s` will need to be a measurable set. The way to express that is `MeasurableSet s`.


## Random variables

A random variable is a measurable function from a measurable space to another.
```lean
variable {Œ© : Type*} [MeasurableSpace Œ©] {X : Œ© ‚Üí ‚Ñù} (hX : Measurable X)
```
In that code we defined a random variable `X` from the measurable space `Œ©` to `‚Ñù` (for which the typeclass inference system finds a measurable space instance). `hX` states that `X` is measurable, which is necessary for most manipulations.

If we define a measure `P` on `Œ©`, we can talk about the law or distribution of `X`. It is by definition the map of the measure `P` by `X`, `P.map X`. There is no specific notation for that law.
To say that `X` is Gaussian with mean 0 and variance 1, write `P.map X = gaussianReal 0 1`.

The expectation of `X` is the integral of that function against the measure `P`, written `‚à´ œâ, X œâ ‚àÇP`.
The notation `P[X]` is shorthand for that expectation. In a `MeasureSpace`, we can further use the notation `ùîº[X]`.

TODO: remark about Lebesgue and Bochner integrals?


## Discrete probability

TODO: `.of_discrete`, `[DiscreteMeasurableSpace]`

TODO: what about PMF? (I don't know anything about those)


## Additional typeclasses on measurable spaces

Some results in probability theory require the sigma-algebra to be the Borel sigma-algebra, generated by the open sets. For example, with the Borel sigma-algebra, continuous functions are measurable.
For that we first need `Œ©` to be a topological space and we then need to add a `[BorelSpace Œ©]` variable.
```lean
variable {Œ© : Type*} [MeasurableSpace Œ©] [TopologicalSpace Œ©] [BorelSpace Œ©]
```

For properties related to conditional distributions and the existence of posterior probability distributions, it is often convenient or necessary to work in a standard Borel space (a measurable space arising as the Borel sets of some Polish topology). See the `StandardBorelSpace` typeclass.
Note that a countable discrete measurable space is a standard Borel space, so there is no need to worry about that typeclass when doing discrete probability.


## Various probability definitions

This section contains pointers to the Mathlib definitions for probability concepts.
That list might be out of date when you read this! Look around in the [documentation](https://leanprover-community.github.io/mathlib4_docs/).


### CDF, pdf, Variance, moments

Here is a list of common concepts about real probability distributions.

- Probability density function of a random variable `X : Œ© ‚Üí E` in a space with measure `P : Measure Œ©`, with respect to measure `Q : Measure E`: `pdf X P Q`
- Cumulative distribution function of `P : Measure ‚Ñù`: `cdf P`. This is a `StieltjesFunction`, a monotone right continuous real function
- Expectation of `X` under `P`: `P[X]`
- Variance: `variance X P`
- Moment of order `p`: `moment X p P`
- Central moment of order `p`: `centralMoment X p P`
- Moment generating function of `X` under `P` at `t : ‚Ñù`: `mgf X P t`
- Cumulant generating function: `cgf X P t`


### Known probability distributions

The Probability/Distributions folder of Mathlib contains several common probability distributions: Exponential, Gamma, Gaussian (only in `‚Ñù`), Geometric, Pareto, Poisson, Uniform.

For example, the Gaussian distribution on the real line with mean `Œº` and variance `v` is a `Measure ‚Ñù`:
```lean
def gaussianReal (Œº : ‚Ñù) (v : ‚Ñù‚â•0) : Measure ‚Ñù := -- omitted
```
The file where the Gaussian is defined also contains properties of its p.d.f.

As another example, to take the uniform distribution on a finite set `s : Set Œ©`, use `uniformOn s : Measure Œ©`.

### Identically distributed

`IdentDistrib X Y P Q` (or `IdentDistrib X Y` in `MeasureSpace`) states that `X` and `Y` are identically distributed.


### Conditioning

TODO: two meanings of conditioning. `cond` vs `condexp` and friends.


### Independence

Mathlib has several definitions for independence. We can talk about independence of random variables, of sets, of sigma-algebras.
The definitions also differ depending on whether we consider only two random variables of an indexed family.
Finally, there are also conditional variants of all those definitions.

#### Unconditional independence

Two independent random variables:
```lean
variable {Œ© : Type*} [MeasurableSpace Œ©] {P : Measure Œ©}
  {X : Œ© ‚Üí ‚Ñù} {Y : Œ© ‚Üí ‚Ñï} (hX : Measurable X) (hY : Measurable Y)
  (hXY : IndepFun X Y P)
```
On a measure space, we can write `IndepFun X Y` without the measure argument.

For family `X : Œπ ‚Üí Œ© ‚Üí ‚Ñù` of independent random variables, use `iIndepFun`.

For sets, use `IndepSet` (two sets) and `iIndepSet` (family of sets). For sigma-algebras, `Indep` and `iIndep`.

#### Conditional independence

TODO

For sets, use `CondIndepSet` (two sets) and `iCondIndepSet` (family of sets). For sigma-algebras, `CondIndep` and `iCondIndep`.

### Martingales, filtrations

A stochastic process with real values (for example) is a function `X : Œπ ‚Üí Œ© ‚Üí ‚Ñù` from an index set `Œπ` to random variables `Œ© ‚Üí ‚Ñù`.

A filtration on the index set can be defined with `‚Ñ± : Filtration Œπ m`, in which `m` is a sigma-algebra on `Œ©`.

We can state that a process `X` is adapted to the filtration `‚Ñ±` with `adapted ‚Ñ± X`.
We can write that it is a martingale with respect to the filtration `‚Ñ±` and the measure `P : Measure Œ©` with `Martingale X ‚Ñ± P`.

A stopping time with respect to a filtration `‚Ñ±` is a function `œÑ : Œ© ‚Üí Œπ` such that for all `i`, the preimage of `{j | j ‚â§ i}` along `œÑ` is measurable with respect to `‚Ñ± i`. 
We can state that `œÑ` is a stopping time with `IsStoppingTime ‚Ñ± œÑ`.

Remark: there is an issue with the current definition of stopping times, which is that if the index set of the filtration `Œπ` is the set of natural numbers `‚Ñï` then `œÑ` has to take values in `‚Ñï`. In particular it cannot be infinite.
That can cause friction with informal stopping times which are often understood to be infinite when they are not well defined.

### Transition kernels

